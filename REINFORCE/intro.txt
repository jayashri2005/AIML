"reinforcement learning" -> means learning from rewards and punishments
i.e robots,deepseek
AlexNet (2012):

8 layers: 5 Conv + 3 FC
Revolutionized ImageNet classification
Used ReLU, Dropout, Data augmentation
Architecture: Conv(11) → Pool → Conv(5) → Pool → Conv(3)×3 → FC×2 → Output
VGG16 (2014):

16 layers: 13 Conv + 3 FC
All 3×3 convolutions, uniform structure
Very deep but simple: Conv(3)×2 → Pool → Conv(3)×2 → Pool → Conv(3)×3 → Pool → FC×3
More parameters than AlexNet
Key differences:

AlexNet: Larger kernels (11×11, 5×5), fewer layers
VGG16: Smaller kernels (3×3), much deeper, more uniform

"transfer learning" -> means using a pre-trained model and fine-tuning it for a specific task
"concept"
Take a model trained on large dataset (like AlexNet,VGG16)
Reuse learned features (edges, textures, patterns)
Fine-tune for your specific task with less data

"how wrk"
Take a model trained on large dataset (like ImageNet)
Reuse learned features (edges, textures, patterns)
Fine-tune for your specific task with less data

"Transfer Learning Process:"

Load Pre-trained Model - ResNet, VGG16 trained on ImageNet
Freeze Early Layers - Keep learned features (edges, textures)

"supervised learning" -> we have input nd output[labels] then train a model
"unsupervised learning"-> we have input but no output[labels] then train a model
"reinforcement learning"-> agent learns through trial and error by taking actions in an environment and receiving rewards or penalties
"semi-supervised"-> combination of supervised and unsupervised learning using both labeled and unlabeled data

"policy optimization formats"

**Policy Gradient Methods:**
- REINFORCE - basic policy gradient algorithm
- Actor-Critic - combines value function and policy
- PPO (Proximal Policy Optimization) - clips policy updates to prevent large changes
- TRPO (Trust Region Policy Optimization) - constrains policy updates within trust region

**Value-Based Methods:**
- Q-Learning - learns action-value function
- Deep Q-Network (DQN) - Q-learning with neural networks
- Double DQN - reduces overestimation bias

**Actor-Critic Variants:**
- A2C/A3C (Advantage Actor-Critic) - uses advantage function
- SAC (Soft Actor-Critic) - maximizes entropy for exploration
- TD3 (Twin Delayed DDPG) - for continuous action spaces

**Used by AI Models:**
- ChatGPT/GPT models: PPO (for fine-tuning from human feedback)
- DeepSeek: PPO + custom optimizations
- Claude: Constitutional AI + RLHF
- Gemini: Mixed approaches including policy gradients
- Most LLMs: PPO for RLHF (Reinforcement Learning from Human Feedback)

applications of Reinforcement learning:
- Game playing (Chess, Go, Atari games)
- Robotics (Control, Navigation)
- Autonomous vehicles (Self-driving cars)
- Resource management (Energy, Cloud computing)
- Finance (Algorithmic trading)
- Healthcare (Treatment planning)
- Recommendation systems
- Natural language processing (Chatbots, Language models)
- Industrial automation
- Sports (Strategy optimization)
- Gaming (NPC behavior, game design)

**OpenAI RL Environments:**

**Gym/Gymnasium:**
- Library of multiple environments for RL testing
- Standardized API for different environments
- Easy to compare algorithms

**Popular Environments:**

**Frozen Lake:**
- Grid world with slippery ice
- Goal: reach goal without falling in holes
- Discrete actions: up, down, left, right
- Good for learning basic policy optimization

**CartPole:**
- Balance pole on moving cart
- Continuous state, discrete actions
- Goal: keep pole balanced as long as possible
- Classic control problem for beginners

**Other Classic Environments:**
- MountainCar - reach the flag with limited power
- Acrobot - swing up a two-link robot arm
- LunarLander - land spacecraft safely
- Atari games - classic arcade games
- MuJoCo - physics simulation for robotics

**Why use these environments:**
- Standardized benchmarks
- Fast simulation (no real robots needed)
- Easy to test and debug algorithms
- Compare results with published papers


"DeepQLearning" 
We use Deep Q‑Learning when the state space is too large, too complex, or continuous for a Q‑table;
and we need a neural network to approximate the Q‑function.
-example: CartPole
input nd target - use same nn but target will not be trained instead of it just copy it 

"""
cartpole -> A pole is attached to a cart that moves along a track. The goal is to balance the pole upright by pushing the cart left or right.
          ->+1 for every timestep the pole stays balanced
          ->-1 if the pole falls
action space -> 0 (left push) 1(right push)
observation space -> [cart_position(x), cart_velocity(x_dot), pole_angle(theta), pole_angular_velocity(theta_dot)]

Parameter           | Termination | Limit
--------------------|-------------|------------------
Cart Position (x)   | Yes         | ±2.4
Pole Angle (θ)      | Yes         | ±12° (~0.2095 rad)
Cart Velocity (ẋ)  | No          | Infinite allowed
Pole Angular Velocity (θ̇) | No    | Infinite allowed
Time Steps          | Yes (Truncation) | 500 (v1)

"""

"UNSQUEEZE" -> expand and  add dimension
"SQUEEZE" -> remove tensor size

Qvalu = state + action info

prob - dev around area; any num in this reg we cal area,prob=0.5 it cover 50 per
math - data lying in middle,mean in the middle 

"likelehood" -> max = mean[may/may not be] 
max likelihood = peak of distribution
Peak of distribution: The most probable parameter value given the data
Not always the mean: For skewed distributions, the maximum likelihood estimate differs from the mean

TD(0) = Q(s,a)<-Q(s,a)+alpha[r+gamma*Qmax(s',a')-Q(s,a)] = qlearning 
TD(0) = Q(s,a)<-Q(s,a)+alpha[r+gamma*Q(s',a')-Q(s,a)] = sarsa 

1 step at a tie - update polcy [T[0]] [chess]
""Monte Carlo"" - update policy [T[1]] 

take all step until game finish then update or see reward = T[1] = "Mounte Carlo"

"policy"=>exp reward ; weighted as multi prob ; expectation or reward that shape our policy ; 
        =>probability take in distribution formats
"weightage" in linear reg point of view = coefficients/parameters that determine feature importance 

"policy gradien asent" = always +ve theta+policy theta->parameter 

"advantage function" = how much better is action a in state s vs average action in state s
                     => reduce noise in Q 
formula 

   => A(s,a)=Q(s,a)-V(s) [Q-V]=>prob reduced 

    where:
    Q(s,a) = expected reward for taking action a in state s
    V(s) = expected reward for being in state s (average over all actions)

E_π[A^π(s,a)] = 0    ← Zero mean (baseline effect)
A^π(s, π(s)) ≈ 0     ← Policy actions have ~0 advantage
max_a A^π(s,a) ≥ 0   ← Best action always non-negative

   

-> Policy Gradient Update: ∇θJ = E[ ∇θlogπ(a|s) * G ]  [PI - prob of policy/action]

->  ∇θlogπ(a|s) * G = [weightage] × [probability] × [reward]

->  π(a|s) = probability of taking action a in state s
     ↓ Variance HIGH (G fluctuates a lot)
     → High variance because G depends on entire episode trajectory
     → One episode might get G=100, another G=5 → unstable updates

∇θJ = E[ ∇θlogπ(a|s) * A(s,a) ]  ← Variance LOW!

Aᵗ = Gᵗ - V(sₜ)  [+ve increase prob -ve reduce chance of getting action]
Aᵍᵃᵉ = δᵗ + γλδᵗ⁺¹
δₜ = rₜ₊₁ + γV(sₜ₊₁) - V(sₜ)


G₀ = r₁ + γr₂ + γ²r₃ + ... = Σᵢ γⁱ⁻¹ rᵢ

where:
G₀ = return from time step 0
r₁, r₂, r₃ = rewards at time steps 1, 2, 3
γ = discount factor (0 < γ < 1)
Σ = summation over all future rewards



where:
Aᵗ = advantage at time t
Gᵗ = return (cumulative reward) from time t to end of episode [full reward/td[1]]
V(sₜ) = value function at state sₜ
δₜ = temporal difference error at time t
rₜ₊₁ = reward at time t+1
γ = discount factor (0.99)
λ = GAE parameter (0.95)

expected val =rew*prob+rew*prob+..
reward taken as V [Vmax,Vavg] or Q


graph summa point pana jurkey movement athuvey avg use pana smooth 


sometime adv fun cal as 
    1.a=q-V
    2.a=g-V
    3.a=v 


"Policy Gradient Theorem" => used when action are continuous ; env used for policy gradient 

"policy gradient" => probability distribution separately calculated for each action

"ActorCritic" -> combines policy gradient (actor) + value function (critic) ; actor selects actions, critic evaluates them 